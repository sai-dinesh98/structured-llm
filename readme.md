Structured LLM (LangGraph-based)

Reliable structured outputs from local LLMs using schema validation, retries, and self-repair.

structured-llm is a lightweight Python package that guarantees schema-correct outputs from LLMs that do not natively support structured output (e.g. local HuggingFace models like Phi-3, Gemma, Mistral).

It uses LangGraph to:

Generate LLM output

Validate it against a Pydantic schema

Automatically retry + repair invalid JSON outputs

Return a strongly-typed Python object

âœ¨ Why this exists

Most local LLMs:

âŒ Do not support function calling

âŒ Do not enforce JSON schemas

âŒ Often return malformed JSON

This package gives you:

âœ… Schema-validated outputs
âœ… Automatic retries
âœ… Self-repair prompts
âœ… Model-agnostic design
âœ… Works with local + hosted LLMs

ğŸ§  How it works (high-level)
Prompt
  â†“
LLM Node
  â†“
Parse Node (Pydantic)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Valid?        â”‚â”€â”€ yes â”€â”€â–¶ DONE
â”‚               â”‚
â”‚ Invalid?      â”‚â”€â”€ no â”€â”€â–¶ Repair Node â†’ retry
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Powered by LangGraph state machines, not brittle regex parsing.

ğŸ“¦ Installation
Editable install (recommended for development)
git clone https://github.com/<your-username>/structured_llm.git
cd structured_llm
pip install -e .


After this, you can import it from any directory.

ğŸ—‚ Project structure
structured_llm/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â””â”€â”€ structured_llm/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ engine.py        # High-level API
        â”œâ”€â”€ graph.py         # LangGraph definition
        â”œâ”€â”€ llm.py           # LLM adapters
        â”œâ”€â”€ nodes.py         # llm / parse / repair nodes
        â”œâ”€â”€ state.py         # Graph state
        â””â”€â”€ utils.py

ğŸš€ Quick start
1ï¸âƒ£ Define a schema
from pydantic import BaseModel, Field

class EvaluationSchema(BaseModel):
    feedback: str = Field(description="Feedback within 100 words")
    score: int = Field(ge=0, le=10)

2ï¸âƒ£ Load your local LLM
from structured_llm.llm import GetGemmaModel

model = GetGemmaModel(
    model_path="path/to/Phi3-mini-instruct"
).get_model()


Any HuggingFace causal LM works.

3ï¸âƒ£ Run structured evaluation
from structured_llm.engine import StructuredEvaluator

evaluator = StructuredEvaluator(
    llm=model,
    schema=EvaluationSchema,
    max_retries=2
)

result = evaluator.invoke(
    prompt="Evaluate the language quality of the following essay...",
)

4ï¸âƒ£ Access typed output
print(result.parsed.feedback)
print(result.parsed.score)


âœ… result.parsed is a real Pydantic object, not raw JSON.

ğŸ§ª What happens on bad JSON?

If the model outputs something like:

Sure! Here's the evaluation:
{
  feedback: "Great essay"
  score: 8


The system will automatically:

Detect schema failure

Generate a repair prompt

Retry with corrected JSON

Stop after max_retries

No manual parsing. No crashes.

ğŸ”§ Configuration
Parameter	Description
schema	Pydantic model defining output
max_retries	Max repair attempts
llm	Any LangChain-compatible chat model
âŒ What this does NOT do

âŒ Does not rely on OpenAI / Anthropic

âŒ Does not require function calling

âŒ Does not use fragile regex parsing

ğŸ§© When should I use this?

Use structured-llm when:

You run local models

You need guaranteed JSON

You want schema safety

You donâ€™t trust raw LLM outputs

ğŸ”® Future roadmap

 Generic StructuredLLM base class

 CLI (structured-llm eval essay.txt)

 Streaming support

 Metrics / tracing hooks

 Multi-schema routing

ğŸ“œ License

MIT License

ğŸ™Œ Credits

Built with:

LangGraph

LangChain

Pydantic